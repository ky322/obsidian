Training set $D=\{(\vec{x}_1,y_1),\dots,(\vec{x_n},y_n)\}$ is drawn from some unknown distribution $P(X,Y)$
Pairs are sampled i.i.d $\large P(D)=P((\vec{x}_1,y_1),\dots,(\vec{x}_n,y_n))=\Pi_{\alpha=1}^n P(\vec{x}_\alpha,y_\alpha)$

MLE Estimate is only good if there are many training vectors with the same identical features as $\vec{x}$. This does not happen in high dimensional spaces or continuous $\vec{x}$

## Naive Bayes

Estimating $P(Y)$: Count how many times we observe each outcome in each class

$$P(y | \vec{x})=\frac{P(\vec{x} | y)P(y)}{P(\vec{x})}$$

>[!info]
>**Naive Bayes Assumption** 
>Feature values are independent given the label $$P(\vec{x} | y) = \prod_{\alpha = 1}^{d} P(x_\alpha | y)$$ where $x_\alpha = [\vec{x}]_\alpha$ is the value for a feature $\alpha$

If Naive Bayes assumption holds then we can define the Bayes Classifier as: $$\Large\begin{align} h(\vec{x}) &=
      \operatorname*{argmax}_y P(y | \vec{x}) \\ &=
      \operatorname*{argmax}_y \; \frac{P(\vec{x} | y)P(y)}{P(\vec{x})} \\ &= \operatorname*{argmax}_y \; P(\vec{x} | y) P(y) && \\ &=\operatorname*{argmax}_y \; \prod_{\alpha=1}^{d} P(x_\alpha | y) P(y)\\ &=\operatorname*{argmax}_y \; \sum_{\alpha = 1}^{d} \log(P(x_\alpha | y)) +\log(P(y)) \end{align}$$
      
## Estimating $P([\vec{x}]_\alpha|y)$

1. Categorical Features:
	- Features: $[\mathbf{x}]_\alpha \in \{f_1, f_2, \cdots,f_{K_\alpha}\}$
		- Each feature $\alpha$ falls into one of $K_\alpha$ categories
	- Model $P(x_\alpha|y)$:  $P(x_{\alpha} = j | y=c) = [\theta_{jc}]_{\alpha} \\ \text{ and }\sum_{j=1}^{K_\alpha} [\theta_{jc}]_{\alpha} = 1$
		-  $[\theta_{jc}]_{\alpha}$ is the probability of feature $\alpha$ having value $j$ given label $c$
	- Parameter Estimation: $$\begin{align} [\hat\theta_{jc}]_{\alpha} &= \frac{\sum_{i=1}^{n} I(y_i = c) I(x_{i\alpha} = j) + l}{\sum_{i=1}^{n} I(y_i = c) + lK_\alpha} = \frac{\text{samples with label c with feature } \alpha \text{ value \(j\) }}{\text{\# of samples with label \(c\)}}\end{align}$$
		- $l$ is a smoothing parameter $\rightarrow$ $1=0$ gives us MLE estimation and $l>0$ gives us MAP
		- Generative Model that we are assuming is that data was generated by choosing the the label (comes in a set of $d$ dice for each dimension one). 
	- Prediction: $\Large\operatorname*{argmax}_y \; P(y=c \mid \mathbf{x}) \propto \operatorname*{argmax}_y \; \hat\pi_c \prod_{\alpha = 1}^{d}    [\hat\theta_{jc}]_\alpha$
	- For $d$ dimensional data, there exists $d$ independent dice for each class. Each feature has one die per class. Training samples were generated by rolling one die after another. The value in dimension $i$ corresponds to the outcome rolled with the $i^{th}$ die 
		![[Bayes Classifier Categorical.png]] 
2. Multinomial features
	- Features:  $\begin{align} x_\alpha \in \{0, 1, 2, \dots, m\} \text{ and } m =    \sum_{\alpha = 1}^d x_\alpha \end{align}$
		- Each feature $\alpha$ represents a count and $m$ is the length of the sequence
	- Model $P(\vec{x}|y)$: $P(\mathbf{x} \mid m, y=c) =    \frac{m!}{x_1! \cdot x_2! \cdot \dots \cdot x_d!} \prod_{\alpha = 1}^d    \left(\theta_{\alpha c}\right)^{x_\alpha}$
	- Parameter Estimation: $$\begin{align} \hat\theta_{\alpha c} = \frac{\sum_{i = 1}^{n} I(y_i = c)    x_{i\alpha} + l}{\sum_{i=1}^{n} I(y_i = c) m_i + l \cdot d } \end{align} = \frac{\text{Times word } \alpha \text{ appears in all
    spam}}{\text{Total words in all spam  }}$$
	- Prediction:  $\operatorname*{argmax}_c \; P(y = c \mid \mathbf{x}) \propto    \operatorname*{argmax}_c \; \hat\pi_c \prod_{\alpha = 1}^d    \hat\theta_{\alpha c}^{x_\alpha}$
	- There are only as many dice as clsases. Each die has $d$ sides and the value of the $ith$ feature shows how many times this side was rolled
		![[Bayes Classifier Multi.png]]
3. Continous Features/ Guassian Naive Bayes
	- Features: $x_\alpha\in\mathbb{R}$
	- Model: $P(x_\alpha|y)$: $\begin{align} P(x_\alpha \mid y=c) = \mathcal{N}\left(\mu_{\alpha c},      \sigma^{2}_{\alpha c}\right) = \frac{1}{\sqrt{2 \pi} \sigma_{\alpha c}}      e^{-\frac{1}{2} \left(\frac{x_\alpha - \mu_{\alpha c}}{\sigma_{\alpha      c}}\right)^2} \end{align}$
	- Parameter Estimation: Estimate parameters of distribution for each dimension and class independently
		- Gaussian distribution have two parameters: mean and variance $$\begin{align} \mu_{\alpha c} &\leftarrow
      \frac{1}{n_c} \sum_{i = 1}^{n} I(y_i = c) x_{i\alpha} &&
      \text{where \(n_c = \sum_{i=1}^{n} I(y_i = c)\)} \\ \sigma_{\alpha c}^2
      &\leftarrow \frac{1}{n_c} \sum_{i=1}^{n} I(y_i = c)(x_{i\alpha} -
      \mu_{\alpha c})^2 \end{align}$$
	  - Each class conditional feature distribution $P(x_\alpha|y)$ is assumed to be from an independent Gaussian distribution with its own mean and variance 
	   ![[Bayes Classifier Continuous.png]]
  
## Naive Bayes is a Linear Classifier

Suppose $y_i \in\{=1,+1\}$ and the features are multinomal

Show that $\Large h(\vec{x}) = \operatorname*{argmax}_y \; P(y)      \prod_{\alpha - 1}^d P(x_\alpha \mid y) = \textrm{sign}(\vec{w}^\top\vec{x} + b)$
$$\mathbf{w}^\top \mathbf{x} + b > 0\Longleftrightarrow h(\mathbf{x}) = +1$$
If we have Gaussian Naive Bayes with constance variance we can show: $$\Large P(y \mid \vec{x}) = \frac{1}{1 + e^{-y (\vec{w}^\top
      \vec{x} +b) }}$$ which is logistic regression. Naive Bayes and Logistic Regression produce asymptotixally the same model if the Naive Bayes assumption holds.