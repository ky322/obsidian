- A point estimate of a parameter $\theta$ is a single number that can be a sensible value for $\theta$
- The selected statistic is the point estimator of $\theta$
- A point estimator $\hat{\theta}$ is unbiased estimator of $\theta$ if $E(\hat{\theta})=\theta$ for every possible value of $\theta$
	- The difference between $E(\hat{\theta})-\theta$ is the bias
- When X is a binomial rv with parameters n and p the sample proportion $\hat{p}=X/n$ is an unbiased estimator of p
- Let $X_1,\cdots X_n$ be a random sample from a distribution with mean and variance then the estimator is unbiased for estimating $\sigma^2$ $$\hat{\sigma^2}=\frac{\sum(X_i-\bar{x})^2}{n-1}$$
### 6.2
![[Pasted image 20231202220008.png]]
- Log Likelihood
- Take the derivative and equating it to zero