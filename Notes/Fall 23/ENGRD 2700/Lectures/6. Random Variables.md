- Random Variable: A number whose value is unknown; a function from the sample space S to $\mathbb{R}$
	- Discrete: The set of possible values that the random variable can take is finite or countably infinite
	- Continuous: The set of possible values that the random variables an take is an interval

### Probability Mass Functions
For each discrete random variable X defined on the sample space S the underlying probability model (S, E, P) induces a probability mass function $p_x(x)$ on the set of possible values of X.
$$p_x(x)=P(X=x)=P(\{s\in S:X(s)=x\})$$$p_x(x) = 0$ if X never takes the value x and $p_x(x)\geq 0$ for all possible values x of X

Geometric Random Variable: The random variable $X:S_2\Rightarrow\{0,1,2,…\}$ that gives the number failures of heads before getting your first tails $p_x(k)=(1-p)p^k$

### Cumulative Distribution Function 
$$F_X(x)=P(X\leq x) = \sum_{y\leq x} p_x(y)$$
Suppose we have a random experiment modeled with the probability model (S,E,P). We are interested in a random variable X on (S,E,P).
1. Observe the experiment
2. Get an outcome $s\in S$ 
3. Get a value $X(s)$ of the random variable

### Expectation
Let (fancy) X be the set of all possible values of the random variable X. Observe the value of X a large number of times $x_1,\cdots, x_M$.
$$\sum_{x\in X}x\cdot p_X(x)$$
Expectation or Mean E(X) of a discrete r.v X $$E(X)=\sum_{x\in X}x\cdot P(X=x)=\sum_{x\in X} x\cdot p_X(x)$$
1. For each possible value of x compute $x\cdot P(X=x)$
2. Sum the values
- E(X) is the balance of the r.v. X’s PMF
- E(X) does not need to be a possible value of X
- E(X) can be infinity

#### Expectation of Functions of Random Variables
Consider a random variable X and a function $h:\mathbb{R}\rightarrow \mathbb{R}$. Then h(x) is also a random variable and $$E(h(X)) = \sum_{x\in X} h(x)P(X=x) = \sum_{x\in X}h(x)p_x(x)$$
#### Properties of Expectation
Expectation of a constant c: $E(c) = c$
Linearity: If X and Y are r.v’s and a and b are constants then $$E(aX+bY) = aE(X) + bE(Y)$$
If h(x) = ax + b$$E(h(x)) = h(E(x)) = aE(x)+b$$
$$E(h(X)) \neq h(E(X))$$
### Variance
- Expectation is a measure of where a r.v.’s values are located. 
- The variance of a r.v. X measures how spread out X’s possible values are around $\mu = E(X)$
- Variance of a r.v. X is $$\sigma^2_X = \text{Var}(X) = E((X-\mu)^2)=\sum_{x\in X}(x-\mu)^2p_X(x)$$
- Standard Deviation of X: $$\sigma_X=\sigma(X) = \sqrt{\text{Var}(X)}$$
- We cant measure how spread out X is by using $\sum_{x\in X}(x-\mu)p_X(x)$ because it always equal 0
- $\sigma^2_x$ measures how hard it is to spin the PMF of X (moment of inertia) around its balance point $E(X)$ (the PMF’s center of mass)
	- More spread out indicates bigger variance
#### Properties of Variance
- If a and b are constants then $$\begin{align}\text{Var}(aX+b)&= E(aX+b-E(aX+b))^2)\\ &=E((aX-aE(X))^2) \\&=a^2E((X-E(X))^2)\\ &= a^2\text{Var}(X)\end{align}$$
- If P(X=c) = 1 for some constant c then $Var(X) = 0$$$Var(X) = E((X-E(X))^2) = (c-c)^2=0$$
- $Var(X) = E(X^2) - E(X)^2$
	- $$\begin{align}Var(X)&= E((X-E(X))^2)\\&=E(X^2-2XE(X)+E(X)^2)\\&=E(X^2)-2E(X)E(X)+E(X^2)\\&=E(X^2)-E(X)^2\end{align}$$
### Standardizing a Random Variable
Suppose X is a r.v with expectation $\mu$ and standard deviation $\sigma$. 
- Standardized Version of X $$X^*=\frac{X-\mu}{\sigma}$$
- X* has expectation 0 and variance 1
	- $E(X^*) = \frac{1}{\sigma}(E(X)-\mu) = 0$
	- $Var(X^*) = \frac{1}{\sigma^2}Var(X) =1$
