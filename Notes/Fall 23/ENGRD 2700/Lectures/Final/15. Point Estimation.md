[[Devore 6]] 
- Point estimation is used to obtain a number that should be close to the quantity we are trying to estimate
- Let $\theta$ denote the number we are trying to estimate and $X_1,\cdots X_n$ denote the sample
- An estimate of $\theta$ is a function $\hat{\theta}(X_1,\cdots X_n)$ of the sample and is a random variable
- We assume we know the distribution of the observations in our simple random sample but do not know the parameters of the distribution
### Evaluating Estimators
- An estimator of the unknown quantity $\theta$ is unbiased if $E(\hat{\theta}(X_1,\cdots X_n))=\theta$
- Estimators are random variables so they have variances $Var(\hat{\theta}(X_1,\cdots X_n))$
	- We prefer estimators with smaller variance
### Maximum Likelihood Estimators
- Use parameter estimates that maximizes the likelihood of seeing data that you actually observed
- Suppose your simple random sample is described by the iid discrete random variables $X_1,\cdots X_n$ with unknown parameter $\theta$
	- Denote the observations by $x_1,\cdots x_n$. The likelihood of seeing this set of data is $l(\theta)=P(X_1=x_1,\cdots,X_n=x_n|\theta)=P(X_1=x_1|\theta)P(X_n=x_n|\theta)$
	- A maximum likelihood estimator of $\theta$ is the value $\hat{\theta}$ that maximizes $l(\theta)$
	- Set the derivative to zero 
- If $X_1,\cdots X_n$ are iid continouse random variables with common PDF $f(x|\theta)$ the likelihood function is $$l(\theta)=f(x_1|\theta)\cdots f(x_n|\theta)$$