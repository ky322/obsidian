- Evaluation of ranked retrieval systems: Intuition, Precision, Recall, Precision@K, Recall@K, Mean Average Precision.
### IR System Evaluation
- Relevance is assessed relative to an information need not a query
	- Document relevant if addresses stated information needed not just cause it contains all words in query
### Evaluation of Unranked Retrieval Sets
- Precision: Fraction of received documents that are relevant
	- $\text{Precision} = \frac{\text{Number of relevant items received}}{\text{Number of retreived items}} = P(\text{relevant|retrieved})$
	- $P=\frac{tp}{tp+fp}$
- Recall: Fraction of relevant documents retrieved
	- $\text{Recall} = \frac{\text{Number of relevant items received}}{\text{Number of relevant items}} = P(\text{retrieved|relevant})$
	- $R= \frac{tp}{tp+fn}$
- Retrieved
	- Relevant: True positive
	- Nonrelevant: False positive
- Not Retrieved
	- Relevant: False Negative
	- Nonrelevant: True negative
- Accuracy: Fraction of classifications that are correct
	- $\frac{tp+tn}{tp+fp+fn+tn}$
	- Do not use
- Precision: Every result on first page to be relevant but not at every relevant document
- Recall: paralegal want every document and tolerate low precision
- F measure: precision vs recall tradeoff
	- Weighted harmonic mean of precision and recall $$F=\frac{1}{\alpha\frac 1 P+(1-\alpha)\frac 1 R}=\frac{(\beta^2+1)PR}{\beta^2P+R}\text{ where }\beta^2=\frac{1-\alpha}{\alpha}$$
	- $\alpha\in[0,1], \beta^2\in[0,\infty]$
- $F_{\beta=1}$: Default balanced F measure equally weights precision and recall so $\alpha=\frac 1 2, \beta=1$
	- $\frac{2PR}{P+R}$
	- $\beta<1$: Emphasize precision
	- $\beta>1$: Emphasize recall
- When the values of two numbers differ greatly the harmonic mean is closer to their minimum than to their mean
### Evaluation of Ranked Retrieval Results
- For a set of relevant documents for an information need $q_j\in Q = \{d_1,\cdots, d_{mj}\}$
- $R_{jk}$: Set of ranked retrieval results from top result until document $d_k$
- $MAP(Q)=\frac 1 {|Q|}\sum_{j=1}^{|Q|}\frac{1}{m_j}\sum_{k=1}^{m_j}\text{Precision}(R_{jk})$
	- When a relevant document is not retrieved, precision value is 0
- MAP is roughly the average under the precision recall curve for a set of queries
- R-precision: Set of known relevant documents Rel and precision of top Rel documents returned
	- Adjusts for size of the set of relevant documents
- Precision and recall: $\frac{r}{|Rel|}$
	- Break even point
- Precision at k and R-precision describe one point on the precision recall curve

Omit 8.5 
