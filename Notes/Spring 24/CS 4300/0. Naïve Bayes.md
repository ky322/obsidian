- Smoothing $P(\Psi_i(d)|c)=\frac{\text{Number of doc with }\Psi_i(D)+1} {\text{Number of documents in c}}$
- $P(A)+P(A')=1$
- $P(A|C)+P(A'|C)=1$
- $P(\Psi_i(d)=1|C)+P(\Psi_i(d)=0|C)=1$
- Binary
	- $\Psi_i(d)=e_i=\begin{cases}1, t_i\in d \\ 0, ti\notin d\end{cases}$
	- $d=[\Psi_1(d),\cdots,\Psi_m(d)]$ where m is size of vocab
	- $\gamma(d)=\arg\max P(c)\cdot P(d|c)$
1. Select a class c with $P(c)$
2. Generate a document with features i according to $P(\Psi_i(d)=e_i)$
### Multinomial Representation
- $d=<yeah, this, is , fantastic>$
- $d=<t_1,t_2,\cdots, t_{md}>$ where $md=$ length of d
Generative
1. Choose class with $P(c)$
2. Given c generate document according to $P(d|c)=P(t_1,\cdots, t_{md}|c)= \prod p(x_k=t_k|c)$
	- $x_k:$ dice rolled
Positional Independence Assumption: $\forall_k,j=P(x_k=t)=P(x_j=t)$
$P(d|c)=\prod_{k=1}^{md}P(x_k=t_k|c)$
$P(d|c)=_{PIA}\prod_{k=1}^{md}P(x=t_k|c)$
$P(d|c)=\prod_{t\in v}P(x=t|c)^{TF_t}$
$w_1x_1+w_2x-b=0$ where w are weights and b is bias
$w_1x_1+w_2x-b>0\to$ Rob
$w_1x_1+w_2x-b<0\to$ Kim
Multinomial NaÃ¯ve Bayes is Linear
$P(e_i|d)>P(e_2|d)$
$\frac{P(c_1|d)}{P(c_2|d)}>1$
$\frac{P(c_1)\cdot\prod_tP(x=t|c_1)^{TF_t}}{P(c_2)\cdot\prod_tP(x=t|c_2)^{TF_t}}>1$
$\log(\frac{P(c_1)}{P(c_2)})+\sum TF_k\log(\frac{P(x=t|c_1)}{P(x=t|c_2)})>0$
