### Platform Decision Making About Content
- Necessary and impactful choices about how to rank and display content and how much to personalize content to users
- Another set of platform decisions: how do platforms moderate undesirable content
- Law does little to constrain speech online and under Section 230 they can legally moderate users' content as much as they want and in any way they want
	- not at all, biased way, and arbitrary and haphazard
### Why moderate
- They legally do not have to
- Ideological commitments
- Happy users = more engagement
- Happy advertisers: the business model of the internet
- To prevent big-P policy (preserve 230 protections)
- External pressures
- Context, purpose, audience, modality matter for a platform's choice in moderation
- Moderation: decisions about whether to take down content, how to display it or whether to amplify it
### Content to Moderate
- Online harassment and abuse has been experienced by 41% of Americans
### Hard Content Moderation Policy Choices
- General vs specific
- Rules vs standards
	- Rules: clear but inflexible
	- Standards: Flexible but allows more discretion less consistently interpreted and administrable
- Are groups treated the same with respect to hate speech?
	- Facebook policy pre Dec 2020: All protected categories treated the same (race, sex, gender, religion, national origin, ethnicity, sexual orientation, disease)
	- Subset not protected
	- Rules do not take into account discrimination or marginalization
	- Dec 2020: Project WoW
	- Spurred in part by civil rights audit and recognition that the most commonly taken down speech consisted of offensive characterizations of white people
	- Comments against white people, men and Americans could still be treated as hate speech but not auto deleted
### Inconsistent Implementation
- Speed and scale makes inevitable
- Human content mods get about 8 seconds per piece of content
### Who Mods?
- Users: reporters or volunteers
- Hired moderators
	- Poorly paid, outsourced to contractors around the world with poor labor protections
- Companies themselves
- AI/Automation
	- Twitter: 0 to 38% detection
	- AI misses cultural, political context, subtle, sarcasm, parody so human judgement
	- Best action flag for human review
- Quasi-governmental appeals bodies
### Ex ante vs ex post
- Ex ante
	- Review all content
	- Preventing people from posting things to begin with
	- Interstitual messages warning people not to post problematic content
- Trade off of ex ante moderation
	- Prevents harm from viewing content
	- Prevent virality
	- Bad UX if people cannot post immediately
	- High overhead
	- Akin to censorship; infringement on speech
	- Limited transparency
- Ex post
	- Reactive; Waits until someone finds out about content
	- Imposes more burden on listeners
	- Process can be slow and impose harm
	- Whack-a-mole to catch new versions of viral content
### False Positive vs False Negatives
- Content moderation to be over inclusive (limits too much speech more false positive) or under inclusive (limits too little speech; more false negatives)
- Posts written in African American English more likely to be misslabled as offensive
- AI tends to misclassify reclaimed language or references to other's hate speech
### Measures and counter measures
- Automated content moderation systems are more sophisticated but people are getting more sophisticated about how to thwart them
	- Misspelling, dog whistles/social steganography